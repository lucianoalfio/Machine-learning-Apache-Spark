{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c5ced5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext,Row\n",
    "from pyspark.streaming import StreamingContext\n",
    "# from pyspark.streaming.flume import FlumeUtils\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "# from pyspark import StorageLevel\n",
    "# from pyspark.storagelevel import StorageLevel\n",
    "#from pyspark.streaming.kafka import KafkaUtils\n",
    "from kafka import KafkaProducer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# machine learning APIs\n",
    "from pyspark.ml.feature import VectorAssembler, Vector\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b39205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/11 05:33:51 WARN Utils: Your hostname, nasa resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/03/11 05:33:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('spark://nasa:7077').appName('bikestreamingapp').config(\"spark.executor.memory\", \"1g\").config('spark.jars', 'mysql-connector-j-8.1.0.jar').config(\"spark.cores.max\", \"2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8e0a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://nasa:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bikestreamingapp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7287ec8b3be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922befc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1)  flume configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85cac855-f328-435d-af4a-baee9e54d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87299775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwx--x 1 hadoop hadoop 768 mar 10 20:17 /home/hadoop/flume/conf/bikestreaming.properties\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/hadoop/flume/conf/bikestreaming.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fc73610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#flume configuration\n",
      "\n",
      "agent1.sources = source1\n",
      "agent1.sinks = sink1\n",
      "agent1.channels = mem1\n",
      "\n",
      "\n",
      "#Define a source for agent1\n",
      "agent1.sources.source1.type=spooldir\n",
      "agent1.sources.source1.spoolDir=/home/hadoop/flume/sent\n",
      "agent1.sources.source1.fileHeader=true\n",
      "agent1.sources.source1.fileSuffix=.SENT\n",
      "\n",
      "\n",
      "#Define sink for agent1 local file system\n",
      "agent1.sinks.sink1.type= file_roll\n",
      "agent1.sinks.sink1.sink.directory= /home/hadoop/edureka/pyspark/certificationproject/streamdata\n",
      "agent1.sinks.sink1.sink.pathManager.prefix = bikestreamdata_\n",
      "\n",
      "#Define a file channel called fileChannel on agent1\n",
      "agent1.channels.mem1.type= memory\n",
      "agent1.channels.mem1.capacity=1000\n",
      "agent1.channels.mem1.transactionCapacity=100\n",
      "\n",
      "#integrations\n",
      "agent1.sources.source1.channels = mem1\n",
      "agent1.sinks.sink1.channel = mem1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /home/hadoop/flume/conf/bikestreaming.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc1f89",
   "metadata": {},
   "source": [
    "# flume setting streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726483b",
   "metadata": {},
   "source": [
    "# send data to spark streaming app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5f6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 hadoop hadoop 617 mar  8 00:59 dataset/bikestreamdata.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l dataset/bikestreamdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e852198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp dataset/bikestreamdata.csv ~/flume/sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e924e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "-rw-rw-r-- 1 hadoop hadoop 617 mar  8 00:59 bikestreamdata.csv.SENT\n"
     ]
    }
   ],
   "source": [
    "!ls -l ~/flume/sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af130e2a",
   "metadata": {},
   "source": [
    "# sink stream data reception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d15170ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 636\n",
      "-rw-rw-r-- 1 hadoop hadoop 648256 mar 11 06:36 bikestreamdata_1712343464545-1\n"
     ]
    }
   ],
   "source": [
    "!ls -l streamdata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5b38e",
   "metadata": {},
   "source": [
    "# 2) spark streaming development and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1debb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType().add('datetime','timestamp').add('season','integer').add('holiday','integer').add('workingday','integer').add('weather','integer').add('temp','double').add('atemp','double').add('humidity','integer').add('windspeed','double').add('casual','integer').add('registered','integer').add('count','integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4243e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set streaming directory in hdfs\n",
    "streamdir='/home/hadoop/edureka/pyspark/certificationproject/streamdata'\n",
    "data = spark.readStream.csv \\\n",
    "(streamdir,schema=myschema,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3c17274-f359-4c10-9191-5e5f23172a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dsf, epoch_id):\n",
    "    \n",
    "    # enconding\n",
    "    categorical_cols=['season','holiday','workingday','weather']\n",
    "    onehotencoder = [OneHotEncoder(inputCol=col, outputCol=f'{col}_ecd') for col in categorical_cols]\n",
    "    \n",
    "    # build pipeline\n",
    "    stage = onehotencoder\n",
    "    pipeline = Pipeline().setStages(stage)\n",
    "    datamodel =pipeline.fit(dsf).transform(dsf)\n",
    "\n",
    "    # season aggregation data\n",
    "    datamodel = datamodel.withColumn('season_1',F.when(F.col('season')==1,1).otherwise(0))\n",
    "    datamodel = datamodel.withColumn('season_2',F.when(F.col('season')==2,2).otherwise(0))\n",
    "    datamodel = datamodel.withColumn('season_3',F.when(F.col('season')==3,3).otherwise(0))\n",
    "    datamodel = datamodel.withColumn('season_4',F.when(F.col('season')==4,4).otherwise(0))\n",
    "    \n",
    "    # weather aggregation data\n",
    "    datamodel = datamodel.withColumn('weather_1',F.when(F.col('weather')==1,1).otherwise(0))\n",
    "    datamodel = datamodel.withColumn('weather_2',F.when(F.col('weather')==2,2).otherwise(0))\n",
    "    datamodel = datamodel.withColumn('weather_3',F.when(F.col('weather')==3,3).otherwise(0))\n",
    "    datamodel = datamodel.withColumn('weather_4',F.when(F.col('weather')==4,4).otherwise(0))\n",
    "    \n",
    "    dfdate = datamodel.withColumn('year', F.year(F.col('datetime'))) \\\n",
    "        .withColumn('month', F.month(F.col('datetime'))) \\\n",
    "        .withColumn('day', F.dayofmonth(F.col('datetime'))) \\\n",
    "        .withColumn('hour', F.hour(F.col('datetime')))\n",
    "     \n",
    "    # print(dfdate.columns)\n",
    "        \n",
    "    # load model\n",
    "    lrmodel = PipelineModel.load('lrmodel')\n",
    "    predictiondata = lrmodel.transform(dfdate)\n",
    "    # predictiondata.printSchema()\n",
    "    predictiondata.select('year','month','day', 'features','prediction').show(truncate=False)\n",
    "    \n",
    "    # data to write in database\n",
    "    dbdata = predictiondata.select('year','month','day','prediction').show(truncate=False)\n",
    "    \n",
    "    # write prediction into RDBMS\n",
    "    print('writing data ni Mysql databases..')\n",
    "    predictiondata.select('year','month','day','prediction').write \\\n",
    "   .mode('overwrite') \\\n",
    "   .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/sparkdb\") \\\n",
    "  .option(\"dbtable\", \"bikestreamprediction\") \\\n",
    "  .option(\"user\", \"hadoop\") \\\n",
    "  .option(\"password\", \"hadoop\") \\\n",
    "  .save()\n",
    "    print('Data writed successfully.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b616353-0bac-4d68-a85d-01eaec0b2f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+---------------------------------------------------------------------------------+------------------+\n",
      "|year|month|day|features                                                                         |prediction        |\n",
      "+----+-----+---+---------------------------------------------------------------------------------+------------------+\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16],[9.84,16.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0])          |16.254671829779   |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[9.02,40.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,1.0])   |40.13419650353228 |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[9.02,32.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,2.0])   |32.2078404506982  |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[9.84,13.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,3.0])   |13.361494657726439|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[9.84,1.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,4.0])    |1.4568736550925792|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,12,14,15,16,17],[9.84,1.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,5.0])    |1.487047501858342 |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[9.02,2.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,6.0])    |2.4915487142621373|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[8.2,3.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,7.0])     |3.4960499266659326|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[9.84,8.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,8.0])    |8.5395327043056   |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[13.12,14.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,9.0])  |14.618059463019222|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[15.58,36.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,10.0]) |36.58940714912052 |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[14.76,56.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,11.0]) |56.49066687307385 |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,11,14,15,16,17],[17.22,84.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,12.0]) |84.4294119838749  |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,12,14,15,16,17],[18.86,94.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,13.0]) |94.4457259487645  |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,12,14,15,16,17],[18.86,106.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,14.0])|106.41069464493006|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,12,14,15,16,17],[18.04,110.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,15.0])|110.3988945696837 |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,12,14,15,16,17],[17.22,93.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,16.0]) |93.50120350798795 |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,12,14,15,16,17],[18.04,67.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,17.0]) |67.6928940528664  |\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,13,14,15,16,17],[17.22,35.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,18.0]) |35.876709429420885|\n",
      "|2011|1    |1  |(18,[2,3,5,8,9,13,14,15,16,17],[17.22,37.0,1.0,1.0,1.0,1.0,2011.0,1.0,1.0,19.0]) |37.89601575108668 |\n",
      "+----+-----+---+---------------------------------------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----+---+------------------+\n",
      "|year|month|day|prediction        |\n",
      "+----+-----+---+------------------+\n",
      "|2011|1    |1  |16.254671829779   |\n",
      "|2011|1    |1  |40.13419650353228 |\n",
      "|2011|1    |1  |32.2078404506982  |\n",
      "|2011|1    |1  |13.361494657726439|\n",
      "|2011|1    |1  |1.4568736550925792|\n",
      "|2011|1    |1  |1.487047501858342 |\n",
      "|2011|1    |1  |2.4915487142621373|\n",
      "|2011|1    |1  |3.4960499266659326|\n",
      "|2011|1    |1  |8.5395327043056   |\n",
      "|2011|1    |1  |14.618059463019222|\n",
      "|2011|1    |1  |36.58940714912052 |\n",
      "|2011|1    |1  |56.49066687307385 |\n",
      "|2011|1    |1  |84.4294119838749  |\n",
      "|2011|1    |1  |94.4457259487645  |\n",
      "|2011|1    |1  |106.41069464493006|\n",
      "|2011|1    |1  |110.3988945696837 |\n",
      "|2011|1    |1  |93.50120350798795 |\n",
      "|2011|1    |1  |67.6928940528664  |\n",
      "|2011|1    |1  |35.876709429420885|\n",
      "|2011|1    |1  |37.89601575108668 |\n",
      "+----+-----+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "writing data ni Mysql databases..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data writed successfully.\n"
     ]
    }
   ],
   "source": [
    "query = data.writeStream \\\n",
    "    .foreachBatch(process) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c444a03-04bf-4ade-89cc-7b913875d6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
